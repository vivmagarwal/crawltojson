This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-02T08:24:58.158Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
src/
  cli.js
  config.js
  crawler.js
  index.js
.gitignore
build.js
package.json
readme.md

================================================================
Repository Files
================================================================

================
File: src/cli.js
================
const { Command } = require("commander");
const { generateConfig } = require("./config.js");
const { crawlWebsite } = require("./crawler.js");
const { readFileSync } = require("fs");
const chalk = require("chalk");

const program = new Command();

program.name("crawltojson").description("Crawl websites and convert them to JSON with ease").version("1.0.4");

program.command("config").description("Generate a configuration file").action(generateConfig);

program
  .command("crawl")
  .description("Crawl website based on config file")
  .action(async () => {
    try {
      const configFile = readFileSync("./crawltojson.config.json", "utf8");
      const config = JSON.parse(configFile);
      await crawlWebsite(config);
    } catch (error) {
      console.error(chalk.red('Error: Could not read config file. Run "crawltojson config" first.'));
      process.exit(1);
    }
  });

program.parse();

================
File: src/config.js
================
const inquirer = require("inquirer");
const { writeFileSync } = require("fs");
const chalk = require("chalk");

const defaultConfig = {
  url: "",
  match: "",
  selector: "",
  maxPages: 50,
  outputFile: "crawltojson.output.json",
  excludePatterns: [],
};

async function generateConfig() {
  console.log(chalk.blue("Creating crawltojson configuration file..."));

  const answers = await inquirer.prompt([
    {
      type: "input",
      name: "url",
      message: "What is the starting URL to crawl?",
      validate: (input) => input.startsWith("http") || "Please enter a valid URL",
    },
    {
      type: "input",
      name: "match",
      message: "What URL pattern should be matched? (e.g., https://example.com/**)",
      default: (answers) => answers.url + "/**",
    },
    {
      type: "input",
      name: "selector",
      message: "What CSS selector should be used to extract content?",
      default: "body",
    },
    {
      type: "number",
      name: "maxPages",
      message: "Maximum number of pages to crawl?",
      default: 50,
    },
  ]);

  const config = { ...defaultConfig, ...answers };

  try {
    writeFileSync("./crawltojson.config.json", JSON.stringify(config, null, 2));
    console.log(chalk.green("Configuration file created successfully!"));
  } catch (error) {
    console.error(chalk.red("Error creating configuration file:", error.message));
    process.exit(1);
  }
}

module.exports = { generateConfig };

================
File: src/crawler.js
================
const { chromium } = require("playwright");
const ora = require("ora");
const chalk = require("chalk");
const { writeFileSync } = require("fs");

async function crawlWebsite(config) {
  const spinner = ora("Starting crawler...").start();
  const results = [];
  let pagesVisited = 0;

  try {
    const browser = await chromium.launch();
    const context = await browser.newContext();
    const page = await context.newPage();

    // Keep track of visited URLs
    const visitedUrls = new Set();

    async function crawlPage(url) {
      if (visitedUrls.has(url) || pagesVisited >= config.maxPages) {
        return;
      }

      try {
        spinner.text = `Crawling ${url}`;
        await page.goto(url);
        visitedUrls.add(url);
        pagesVisited++;

        // Extract content based on selector
        const content = await page.$$eval(config.selector, (elements) => elements.map((el) => el.innerText));

        // Store result
        results.push({
          url,
          content: content.join("\n"),
          timestamp: new Date().toISOString(),
        });

        // Find links matching the pattern
        const links = await page.$$eval("a[href]", (elements, pattern) => elements.map((el) => el.href).filter((href) => href.match(pattern)), config.match.replace("**", ".*"));

        // Crawl found links
        for (const link of links) {
          if (pagesVisited < config.maxPages) {
            await crawlPage(link);
          }
        }
      } catch (error) {
        console.error(chalk.yellow(`Warning: Error crawling ${url}:`, error.message));
      }
    }

    await crawlPage(config.url);
    await browser.close();

    // Save results
    writeFileSync(config.outputFile, JSON.stringify(results, null, 2));

    spinner.succeed(chalk.green(`Crawling complete! Processed ${pagesVisited} pages`));
  } catch (error) {
    spinner.fail(chalk.red("Crawling failed:", error.message));
    process.exit(1);
  }
}

module.exports = { crawlWebsite };

================
File: src/index.js
================
const { crawlWebsite } = require("./crawler.js");
const { generateConfig } = require("./config.js");

module.exports = {
  crawlWebsite,
  generateConfig,
};

================
File: .gitignore
================
# Dependencies
node_modules/

# Build output
dist/

# Logs
*.log
npm-debug.log*

# Runtime data
.DS_Store
*.pem

# IDE
.idea/
.vscode/

# Config files that should be generated locally
crawltojson.config.json
crawltojson.output.json

================
File: build.js
================
// build.js
const esbuild = require("esbuild");
const fs = require("fs/promises");

async function build() {
  try {
    // Common build options
    const buildOptions = {
      bundle: true,
      platform: "node",
      format: "cjs",
      external: ["playwright", "chalk", "inquirer", "commander", "ora"],
    };

    // Build the CLI
    await esbuild.build({
      ...buildOptions,
      entryPoints: ["./src/cli.js"],
      outfile: "./dist/cli.cjs",
    });

    // Build the library
    await esbuild.build({
      ...buildOptions,
      entryPoints: ["./src/index.js"],
      outfile: "./dist/index.cjs",
    });

    // Add shebang to CLI
    const cliContent = await fs.readFile("./dist/cli.cjs", "utf8");
    await fs.writeFile("./dist/cli.cjs", `#!/usr/bin/env node\n${cliContent}`);
    await fs.chmod("./dist/cli.cjs", 0o755);

    console.log("Build complete");
  } catch (error) {
    console.error("Build failed:", error);
    process.exit(1);
  }
}

build();

================
File: package.json
================
{
  "name": "crawltojson",
  "version": "1.0.5",
  "description": "Crawl websites and convert them to JSON with ease",
  "type": "commonjs",
  "main": "./dist/index.cjs",
  "bin": {
    "crawltojson": "./dist/cli.cjs"
  },
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "node build.js",
    "prepare": "npm run build",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "dependencies": {
    "chalk": "^4.1.2",
    "commander": "^11.1.0",
    "inquirer": "^8.2.6",
    "ora": "^5.4.1",
    "playwright": "^1.40.0"
  },
  "devDependencies": {
    "esbuild": "^0.19.8"
  },
  "engines": {
    "node": ">=14.16"
  },
  "keywords": [
    "crawler",
    "web-scraping",
    "json",
    "cli",
    "website-crawler"
  ],
  "author": "",
  "license": "MIT"
}

================
File: readme.md
================
# crawltojson

Crawl websites and convert them to structured JSON with ease. Perfect for creating training data, content migration, or web scraping.

## Quick Start (For Users)

### Installation

```bash
npm install -g crawltojson
```

Or use without installing via npx:
```bash
npx crawltojson
```

### Usage

1. Generate configuration:
```bash
crawltojson config
```

2. Start crawling:
```bash
crawltojson crawl
```

## Development & Publishing

### Local Development

1. Clone the repository:
```bash
git clone https://github.com/yourusername/crawltojson.git
cd crawltojson
```

2. Install dependencies:
```bash
npm install
```

3. Build the project:
```bash
npm run build
```

4. Link for local testing:
```bash
npm link
crawltojson --help
```

### Making Changes

1. Watch mode for development:
```bash
npm run dev
```

2. Rebuild after changes:
```bash
npm run build
```

### Publishing to npm

1. Login to npm (if not already):
```bash
npm login
```

2. Clean and rebuild:
```bash
rm -rf node_modules dist package-lock.json
npm install
npm run build
```

3. Publish to npm:
```bash
npm publish
```

## Configuration Options

The configuration wizard will help you set up:

- `url`: Starting URL to crawl
- `match`: URL pattern to match (supports glob patterns)
- `selector`: CSS selector to extract content
- `maxPages`: Maximum number of pages to crawl
- `outputFile`: Name of the output JSON file (default: crawltojson.output.json)

## Example

```bash
# Generate config
npx crawltojson config

# Follow the prompts:
# URL: https://example.com
# Pattern: https://example.com/**
# Selector: article
# Max Pages: 100

# Start crawling
npx crawltojson crawl
```

## Output Format

The generated JSON file will contain an array of objects:

```json
[
  {
    "url": "https://example.com/page1",
    "content": "Extracted content...",
    "timestamp": "2024-11-02T12:00:00.000Z"
  }
]
```

## Common Issues & Troubleshooting

### Clean Install
If you encounter any issues, try a clean install:
```bash
rm -rf node_modules dist package-lock.json
npm install
npm run build
```

### Permission Issues
If you get permission errors when running the CLI:
```bash
chmod +x ./dist/cli.js
```

### Build Issues
If the build fails with module resolution errors:
```bash
# Check node version (should be >=14.16)
node --version

# Clear npm cache
npm cache clean --force

# Try rebuilding
npm run build
```

## License

MIT
