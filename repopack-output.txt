This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-02T12:10:09.701Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
src/
  cli.js
  config.js
  crawler.js
  index.js
  utils.js
.gitignore
build.js
package.json
readme.md

================================================================
Repository Files
================================================================

================
File: src/cli.js
================
const { Command } = require("commander");
const { generateConfig } = require("./config.js");
const { crawlWebsite } = require("./crawler.js");
const { readFileSync } = require("fs");
const chalk = require("chalk");

const program = new Command();

program.name("crawltojson").description("Crawl websites and convert them to JSON with ease").version("1.0.4");

program.command("config").description("Generate a configuration file").action(generateConfig);

program
  .command("crawl")
  .description("Crawl website based on config file")
  .action(async () => {
    try {
      const configFile = readFileSync("./crawltojson.config.json", "utf8");
      const config = JSON.parse(configFile);
      await crawlWebsite(config);
    } catch (error) {
      console.error(chalk.red('Error: Could not read config file. Run "crawltojson config" first.'));
      process.exit(1);
    }
  });

program.parse();

================
File: src/config.js
================
const inquirer = require("inquirer");
const { writeFileSync } = require("fs");
const chalk = require("chalk");

const defaultConfig = {
  url: "",
  match: "",
  selector: "",
  maxPages: 50,
  outputFile: "crawltojson.output.json",
  excludePatterns: [
    "**/tag/**", // Ignore tag pages
    "**/tags/**", // Ignore tags pages
    "**/#*", // Ignore anchor links
    "**/search**", // Ignore search pages
    "**.pdf", // Ignore PDF files
    "**/archive/**", // Ignore archive pages
  ],
};

async function generateConfig() {
  console.log(chalk.blue("Creating crawltojson configuration file..."));

  const answers = await inquirer.prompt([
    {
      type: "input",
      name: "url",
      message: "What is the starting URL to crawl?",
      validate: (input) => input.startsWith("http") || "Please enter a valid URL",
    },
    {
      type: "input",
      name: "match",
      message: "What URL pattern should be matched? (e.g., https://example.com/**)",
      default: (answers) => {
        const baseUrl = answers.url.replace(/\/+$/, "");
        return `${baseUrl}/**`;
      },
    },
    {
      type: "input",
      name: "selector",
      message: "What CSS selector should be used to extract content?",
      default: "body",
    },
    {
      type: "number",
      name: "maxPages",
      message: "Maximum number of pages to crawl?",
      default: 50,
    },
  ]);

  const config = { ...defaultConfig, ...answers };

  try {
    writeFileSync("./crawltojson.config.json", JSON.stringify(config, null, 2));
    console.log(chalk.green("Configuration file created successfully!"));
  } catch (error) {
    console.error(chalk.red("Error creating configuration file:", error.message));
    process.exit(1);
  }
}

module.exports = { generateConfig };

================
File: src/crawler.js
================
// src/crawler.js
const { chromium } = require("playwright");
const chalk = require("chalk");
const { writeFileSync } = require("fs");
const { spawn, execSync } = require("child_process");
const path = require("path");
const { normalizeUrl, shouldExcludeUrl } = require("./utils.js");

function installBrowser(callback) {
  console.log(chalk.blue("Installing Playwright browser..."));
  try {
    execSync("npx playwright install chromium", { stdio: "inherit" });
    console.log(chalk.green("Browser installed successfully"));
    callback(null, true);
  } catch (error) {
    console.error(chalk.red("Failed to install browser automatically"));
    console.error(chalk.red("\nPlease run this command manually:"));
    console.error(chalk.blue("\n    npx playwright install chromium\n"));
    callback(null, false);
  }
}

function crawlWebsite(config, callback) {
  console.log(chalk.blue("\nStarting crawler...\n"));

  const tempScript = `
    const { chromium } = require('playwright');
    
    function delay(ms) {
      return new Promise(resolve => setTimeout(resolve, ms));
    }

    function getRandomDelay() {
      return Math.floor(Math.random() * (1500 - 500 + 1) + 500);
    }

    function normalizeUrl(url) {
      try {
        const urlObj = new URL(url);
        urlObj.hash = "";
        urlObj.search = "";
        return urlObj.toString().replace(/\/+$/, "");
      } catch (error) {
        return url;
      }
    }
    
    function shouldExcludeUrl(url, patterns) {
      return patterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*\*/g, ".*").replace(/[.*+?^$()|[\]\\]/g, "\\$&"), "i");
        return regex.test(url);
      });
    }
    
    function crawl() {
      const results = [];
      let pagesVisited = 0;
      const visitedUrls = new Set();
      const excludePatterns = ${JSON.stringify(config.excludePatterns || [])};
      
      function crawlPage(browser, context, page, url, callback) {
        const normalizedUrl = normalizeUrl(url);
        
        if (visitedUrls.has(normalizedUrl) || 
            pagesVisited >= ${config.maxPages} || 
            shouldExcludeUrl(normalizedUrl, excludePatterns)) {
          return callback();
        }

        process.send({ type: 'pageStart', url: normalizedUrl });

        delay(getRandomDelay())
          .then(() => page.goto(url))
          .then(() => {
            visitedUrls.add(normalizedUrl);
            pagesVisited++;
            
            return page.$$eval('${config.selector}', 
              (elements) => elements.map((el) => el.innerText)
            );
          })
          .then((content) => {
            results.push({
              url: normalizedUrl,
              content: content.join("\\n"),
              timestamp: new Date().toISOString(),
            });

            process.send({ type: 'pageSaved', url: normalizedUrl });
            
            return page.$$eval(
              "a[href]",
              (elements, pattern) => 
                elements
                  .map((el) => el.href)
                  .filter((href) => href.match(pattern)),
              '${config.match.replace("**", ".*")}'
            );
          })
          .then((links) => {
            let processed = 0;
            
            function processNextLink() {
              if (processed >= links.length || pagesVisited >= ${config.maxPages}) {
                return callback();
              }
              
              crawlPage(browser, context, page, links[processed], function() {
                processed++;
                processNextLink();
              });
            }
            
            processNextLink();
          })
          .catch((error) => {
            process.send({ type: 'pageError', url: normalizedUrl, message: error.message });
            callback();
          });
      }
      
      chromium.launch({ headless: true })
        .then(function(browser) {
          return browser.newContext()
            .then(function(context) {
              return context.newPage()
                .then(function(page) {
                  return { browser, context, page };
                });
            });
        })
        .then(function({ browser, context, page }) {
          crawlPage(browser, context, page, '${config.url}', function() {
            browser.close()
              .then(() => {
                process.send({ 
                  type: 'complete', 
                  results,
                  pagesVisited 
                });
              });
          });
        })
        .catch(function(error) {
          if (error.message.includes("Executable doesn't exist")) {
            process.send({ type: 'needsInstall' });
          } else {
            process.send({ type: 'error', message: error.message });
          }
        });
    }
    
    crawl();
  `;

  const scriptPath = path.join(process.cwd(), ".temp-crawler-script.js");
  writeFileSync(scriptPath, tempScript);

  let retries = 0;
  const maxRetries = 1;

  function runCrawler(cb) {
    const crawler = spawn("node", [scriptPath], {
      stdio: ["inherit", "inherit", "inherit", "ipc"],
    });

    crawler.on("message", function (message) {
      switch (message.type) {
        case "needsInstall":
          if (retries < maxRetries) {
            retries++;
            console.log(chalk.yellow("\nBrowser not found, attempting to install..."));
            installBrowser(function (err, installed) {
              if (installed) {
                console.log(chalk.blue("\nRetrying crawler..."));
                runCrawler(cb);
              } else {
                cb(new Error("Browser installation required"));
              }
            });
          } else {
            cb(new Error("Browser installation required"));
          }
          break;

        case "pageStart":
          console.log(chalk.blue(`→ Starting: ${message.url}`));
          break;

        case "pageSaved":
          console.log(chalk.green(`✓ Saved: ${message.url}`));
          break;

        case "pageError":
          console.log(chalk.red(`✗ Error: ${message.url} - ${message.message}`));
          break;

        case "complete":
          writeFileSync(config.outputFile, JSON.stringify(message.results, null, 2));
          console.log(chalk.green(`\n✓ Crawling complete! Processed ${message.pagesVisited} pages`));
          console.log(chalk.blue(`✓ Results saved to ${config.outputFile}\n`));
          cb();
          break;

        case "error":
          cb(new Error(message.message));
          break;
      }
    });

    crawler.on("error", function (error) {
      cb(error);
    });

    crawler.on("exit", function (code) {
      if (code !== 0) {
        cb(new Error(`Crawler process exited with code ${code}`));
      }
    });
  }

  runCrawler(function (error) {
    try {
      require("fs").unlinkSync(scriptPath);
    } catch (cleanupError) {
      // Ignore cleanup errors
    }

    if (error) {
      console.error(chalk.red("\nCrawling failed:", error.message, "\n"));
      return process.exit(1);
    }
  });
}

module.exports = { crawlWebsite };

================
File: src/index.js
================
// src/index.js
const { crawlWebsite } = require("./crawler.js");
const { generateConfig } = require("./config.js");

module.exports = {
  crawlWebsite,
  generateConfig,
};

================
File: src/utils.js
================
// src/utils.js
/**
 * Converts a glob-style pattern (with ** wildcards) to a RegExp for matching URLs.
 * @param {string} pattern - The glob-style pattern to convert.
 * @returns {RegExp} - The generated regular expression.
 */
function patternToRegex(pattern) {
  return new RegExp(pattern.replace(/\*\*/g, ".*").replace(/[.*+?^$()|[\]\\]/g, "\\$&"), "i");
}

/**
 * Normalizes a URL by removing hash and search parameters, and trailing slashes.
 * @param {string} url - The URL to normalize.
 * @returns {string} - The normalized URL.
 */
function normalizeUrl(url) {
  try {
    const urlObj = new URL(url);
    urlObj.hash = "";
    urlObj.search = "";
    return urlObj.toString().replace(/\/+$/, "");
  } catch (error) {
    console.error("Error normalizing URL:", error);
    return url;
  }
}

/**
 * Checks if a URL should be excluded based on the exclude patterns.
 * @param {string} url - The URL to check.
 * @param {string[]} excludePatterns - Array of glob patterns to exclude.
 * @returns {boolean} - True if the URL should be excluded.
 */
function shouldExcludeUrl(url, excludePatterns) {
  return excludePatterns.some((pattern) => {
    const regex = new RegExp(pattern.replace(/\*\*/g, ".*").replace(/[.*+?^$()|[\]\\]/g, "\\$&"), "i");
    return regex.test(url);
  });
}

module.exports = { patternToRegex, normalizeUrl, shouldExcludeUrl };

================
File: .gitignore
================
# Dependencies
node_modules/

# Build output
dist/

# Logs
*.log
npm-debug.log*

# Runtime data
.DS_Store
*.pem

# IDE
.idea/
.vscode/

# Config files that should be generated locally
crawltojson.config.json
crawltojson.output.json

================
File: build.js
================
// build.js
const esbuild = require("esbuild");
const fs = require("fs/promises");

async function build() {
  try {
    // Common build options
    const buildOptions = {
      bundle: true,
      platform: "node",
      format: "cjs",
      external: ["playwright", "chalk", "inquirer", "commander", "ora"],
    };

    // Build the CLI
    await esbuild.build({
      ...buildOptions,
      entryPoints: ["./src/cli.js"],
      outfile: "./dist/cli.cjs",
    });

    // Build the library
    await esbuild.build({
      ...buildOptions,
      entryPoints: ["./src/index.js"],
      outfile: "./dist/index.cjs",
    });

    // Add shebang to CLI
    const cliContent = await fs.readFile("./dist/cli.cjs", "utf8");
    await fs.writeFile("./dist/cli.cjs", `#!/usr/bin/env node\n${cliContent}`);
    await fs.chmod("./dist/cli.cjs", 0o755);

    console.log("Build complete");
  } catch (error) {
    console.error("Build failed:", error);
    process.exit(1);
  }
}

build();

================
File: package.json
================
{
  "name": "crawltojson",
  "version": "1.0.36",
  "description": "Crawl websites and convert them to JSON with ease",
  "type": "commonjs",
  "main": "./dist/index.cjs",
  "bin": {
    "crawltojson": "./dist/cli.cjs"
  },
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "node build.js",
    "prepare": "npm run build",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "dependencies": {
    "chalk": "^4.1.2",
    "commander": "^11.1.0",
    "inquirer": "^8.2.6",
    "ora": "^5.4.1",
    "playwright": "^1.40.0"
  },
  "devDependencies": {
    "esbuild": "^0.19.8"
  },
  "engines": {
    "node": ">=14.16"
  },
  "keywords": [
    "crawler",
    "web-scraping",
    "json",
    "cli",
    "website-crawler"
  ],
  "author": "",
  "license": "MIT"
}

================
File: readme.md
================
# crawltojson

Crawl websites and convert them to structured JSON with ease. Perfect for creating training data, content migration, or web scraping.

## Quick Start (For Users)

### Installation

```bash
npm install -g crawltojson
```

Or use without installing via npx:
```bash
npx crawltojson
```

### Usage

1. Generate configuration:
```bash
crawltojson config
```

2. Start crawling:
```bash
crawltojson crawl
```

## Development & Publishing

### Local Development

1. Clone the repository:
```bash
git clone https://github.com/yourusername/crawltojson.git
cd crawltojson
```

2. Install dependencies:
```bash
npm install
```

3. Build the project:
```bash
npm run build
```

4. Link for local testing:
```bash
npm link
crawltojson --help
```

### Making Changes

1. Watch mode for development:
```bash
npm run dev
```

2. Rebuild after changes:
```bash
npm run build
```

### Publishing to npm

1. Login to npm (if not already):
```bash
npm login
```

2. Clean and rebuild:
```bash
rm -rf node_modules dist package-lock.json
npm install
npm run build
```

3. Publish to npm:
```bash
npm publish
```

## Configuration Options

The configuration wizard will help you set up:

- `url`: Starting URL to crawl
- `match`: URL pattern to match (supports glob patterns)
- `selector`: CSS selector to extract content
- `maxPages`: Maximum number of pages to crawl
- `outputFile`: Name of the output JSON file (default: crawltojson.output.json)

## Example

```bash
# Generate config
npx crawltojson config

# Follow the prompts:
# URL: https://example.com
# Pattern: https://example.com/**
# Selector: article
# Max Pages: 100

# Start crawling
npx crawltojson crawl
```

## Output Format

The generated JSON file will contain an array of objects:

```json
[
  {
    "url": "https://example.com/page1",
    "content": "Extracted content...",
    "timestamp": "2024-11-02T12:00:00.000Z"
  }
]
```

## Common Issues & Troubleshooting

### Clean Install
If you encounter any issues, try a clean install:
```bash
rm -rf node_modules dist package-lock.json
npm install
npm run build
```

### Permission Issues
If you get permission errors when running the CLI:
```bash
chmod +x ./dist/cli.js
```

### Build Issues
If the build fails with module resolution errors:
```bash
# Check node version (should be >=14.16)
node --version

# Clear npm cache
npm cache clean --force

# Try rebuilding
npm run build
```

## License

MIT
