This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-02T07:39:27.967Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
src/
  cli.js
  config.js
  crawler.js
.gitignore
build.js
readme.md

================================================================
Repository Files
================================================================

================
File: src/cli.js
================
// bin/cli.js
#!/usr/bin/env node
import { Command } from 'commander';
import { generateConfig } from '../lib/config.js';
import { crawlWebsite } from '../lib/crawler.js';
import { readFileSync } from 'fs';

const program = new Command();

program
  .name('webtojson')
  .description('Convert websites to JSON with ease')
  .version('1.0.0');

program
  .command('config')
  .description('Generate a configuration file')
  .action(generateConfig);

program
  .command('crawl')
  .description('Crawl website based on config file')
  .action(async () => {
    try {
      const configFile = readFileSync('./webtojson.config.json', 'utf8');
      const config = JSON.parse(configFile);
      await crawlWebsite(config);
    } catch (error) {
      console.error('Error: Could not read config file. Run "webtojson config" first.');
      process.exit(1);
    }
  });

program.parse();

================
File: src/config.js
================
import inquirer from "inquirer";
import { writeFileSync } from "fs";
import chalk from "chalk";

const defaultConfig = {
  url: "",
  match: "",
  selector: "",
  maxPages: 50,
  outputFile: "webtojson.output.json",
  excludePatterns: [],
};

export async function generateConfig() {
  console.log(chalk.blue("Creating webtojson configuration file..."));

  const answers = await inquirer.prompt([
    {
      type: "input",
      name: "url",
      message: "What is the starting URL to crawl?",
      validate: (input) => input.startsWith("http") || "Please enter a valid URL",
    },
    {
      type: "input",
      name: "match",
      message: "What URL pattern should be matched? (e.g., https://example.com/**)",
      default: (answers) => answers.url + "/**",
    },
    {
      type: "input",
      name: "selector",
      message: "What CSS selector should be used to extract content?",
      default: "body",
    },
    {
      type: "number",
      name: "maxPages",
      message: "Maximum number of pages to crawl?",
      default: 50,
    },
  ]);

  const config = { ...defaultConfig, ...answers };

  try {
    writeFileSync("./webtojson.config.json", JSON.stringify(config, null, 2));
    console.log(chalk.green("Configuration file created successfully!"));
  } catch (error) {
    console.error(chalk.red("Error creating configuration file:", error.message));
    process.exit(1);
  }
}

================
File: src/crawler.js
================
import playwright from "playwright";
import ora from "ora";
import chalk from "chalk";
import { writeFileSync } from "fs";

export async function crawlWebsite(config) {
  const spinner = ora("Starting crawler...").start();
  const results = [];
  let pagesVisited = 0;

  try {
    const browser = await playwright.chromium.launch();
    const context = await browser.newContext();
    const page = await context.newPage();

    // Keep track of visited URLs
    const visitedUrls = new Set();

    async function crawlPage(url) {
      if (visitedUrls.has(url) || pagesVisited >= config.maxPages) {
        return;
      }

      try {
        spinner.text = `Crawling ${url}`;
        await page.goto(url);
        visitedUrls.add(url);
        pagesVisited++;

        // Extract content based on selector
        const content = await page.$$eval(config.selector, (elements) => elements.map((el) => el.innerText));

        // Store result
        results.push({
          url,
          content: content.join("\n"),
          timestamp: new Date().toISOString(),
        });

        // Find links matching the pattern
        const links = await page.$$eval("a[href]", (elements, pattern) => elements.map((el) => el.href).filter((href) => href.match(pattern)), config.match.replace("**", ".*"));

        // Crawl found links
        for (const link of links) {
          if (pagesVisited < config.maxPages) {
            await crawlPage(link);
          }
        }
      } catch (error) {
        console.error(chalk.yellow(`Warning: Error crawling ${url}:`, error.message));
      }
    }

    await crawlPage(config.url);
    await browser.close();

    // Save results
    writeFileSync(config.outputFile, JSON.stringify(results, null, 2));

    spinner.succeed(chalk.green(`Crawling complete! Processed ${pagesVisited} pages`));
  } catch (error) {
    spinner.fail(chalk.red("Crawling failed:", error.message));
    process.exit(1);
  }
}

================
File: .gitignore
================
# Dependencies
node_modules/

# Build output
dist/

# Logs
*.log
npm-debug.log*

# Runtime data
.DS_Store
*.pem

# IDE
.idea/
.vscode/

# Config files that should be generated locally
webtojson.config.json
webtojson.output.json

================
File: build.js
================
// package.json
{
  "name": "web-to-json",
  "version": "1.0.0",
  "description": "Convert websites to JSON with ease",
  "type": "module",
  "main": "./dist/index.js",
  "bin": {
    "webtojson": "./dist/cli.js"
  },
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "node build.js",
    "prepare": "npm run build",
    "dev": "node build.js --watch",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "dependencies": {
    "commander": "^11.1.0",
    "playwright": "^1.40.0",
    "chalk": "^5.3.0",
    "ora": "^7.0.1",
    "inquirer": "^9.2.12"
  },
  "devDependencies": {
    "esbuild": "^0.19.8"
  },
  "engines": {
    "node": ">=14.16"
  },
  "keywords": [
    "crawler",
    "web-scraping",
    "json",
    "cli"
  ],
  "author": "",
  "license": "MIT"
}

// build.js
import * as esbuild from 'esbuild';

const isWatch = process.argv.includes('--watch');

/** @type {import('esbuild').BuildOptions} */
const buildOptions = {
  entryPoints: ['./src/cli.js', './src/index.js'],
  bundle: true,
  platform: 'node',
  format: 'esm',
  outdir: './dist',
  banner: {
    js: '#!/usr/bin/env node',
  },
};

if (isWatch) {
  const context = await esbuild.context(buildOptions);
  await context.watch();
  console.log('Watching for changes...');
} else {
  await esbuild.build(buildOptions);
  console.log('Build complete');
}

================
File: readme.md
================
## Readme here for the project
```
